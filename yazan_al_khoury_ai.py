# -*- coding: utf-8 -*-
"""Yazan_Al_Khoury_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tSh1QOQf561R1XdPDx96pjeetSCdzPBB

# **Importing Libraries**
"""

import pandas as pd #preprocessing and dealing with CSVs and data and for reading data etc
import numpy as np
import matplotlib.pyplot as plt #for plots (like the pie plot we used)
import seaborn as sns
import kagglehub # for uploading the dataset from kaggle
from sklearn.preprocessing import LabelEncoder # is for trasnforming categorical data into numbers that the algorithms can understand
from sklearn.model_selection import train_test_split # for splitting the data (df) to training and testing
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler # for scaling the data within a specefic range so no bias happens when training to bigger feature values
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import os # to see what is inside folders

"""# **Uploading Dataset**"""

#from kaggle
path = kagglehub.dataset_download("shashwatwork/web-page-phishing-detection-dataset")

print("Path to dataset files:", path)

import os
files_in_dir = os.listdir(path)
print("Files in dataset directory:", files_in_dir)

df=pd.read_csv(path+'/dataset_phishing.csv') #we saved the data in df
df.head() #.head gives the first 5 rows of the data

"""# **Checking Dataset**"""

df.head()

#shape of the dataset
df.shape # how many rows and columns(features)c in the dataset

#status is the target (y)
#check dist using a pieplot
#we can see the target is balanced

status_counts = df['status'].value_counts()

plt.figure(figsize=(5, 5))
plt.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette('pastel'))
plt.title('Distribution of Website Status (Legitimate vs. Phishing)')
plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

#check the datatypes of the columns
#we can see that all is numerical but URL and the target
#for the URL it will be removed
#for the target we will use binary encoding
df.info()

#check for null values
#we can see we have no missing (null) values
df.isnull().sum().sum()

#check for 0 variance columns --> no meaningful patterns to learn
zero_variance_columns = df.columns[df.nunique() == 1] #nunique --> number of unique values in this column
print("Columns with 0 variance:", zero_variance_columns)

#remove them
df.drop(zero_variance_columns, axis=1, inplace=True) #axis=1 refers to deleting columns

#check shape after
#we can see that 6 columns were removed
df.shape

"""# **Data Encoding**"""

#save URLs for later
urls = df['url']

#remove the URL column
df.drop('url', axis=1, inplace=True)

#encode the target
#legitimate is 0 and phishing is 1
le = LabelEncoder()
df['status'] = le.fit_transform(df['status'])
df['status'].head()

df.info()

"""# **ML**

## Call for Action

Comparing the models, the Random Forest Classifier demonstrated the strongest performance with the highest accuracy (0.9676), precision (0.9748), recall (0.9593), and F1-score (0.9670) on the test data. The Logistic Regression model also performed well (accuracy 0.9558), while the Decision Tree had the lowest test performance (accuracy 0.9361) and showed signs of overfitting on the training data.

## **Data Splitting**
"""

df.head()

#split the data to X and Y
X = df.drop('status', axis=1)
y = df['status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # random state guarantees that the same rows show each time for traininng and testing splits

#check the shapes of each
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

"""##**Feature Scaling**"""

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""## **Logistic Regression**"""

lr= LogisticRegression() # for binary classification either phishing or legitimate
lr.fit(X_train_scaled, y_train) # fit = train --> it is like training for a math exam

#traning data results
y_pred_train = lr.predict(X_train_scaled)
accuracy_train = accuracy_score(y_train, y_pred_train)
precision_train = precision_score(y_train, y_pred_train)
recall_train = recall_score(y_train, y_pred_train)
f1_train = f1_score(y_train, y_pred_train)

print("Training Data Results:")
print(f"Accuracy: {accuracy_train}")
print(f"Precision: {precision_train}")
print(f"Recall: {recall_train}")
print(f"F1 Score: {f1_train}")

#testing data results
y_pred_test = lr.predict(X_test_scaled)
accuracy_test = accuracy_score(y_test, y_pred_test)
precision_test = precision_score(y_test, y_pred_test)
recall_test = recall_score(y_test, y_pred_test)
f1_test = f1_score(y_test, y_pred_test)

print("Testing Data Results:")
print(f"Accuracy: {accuracy_test}")
print(f"Precision: {precision_test}")
print(f"Recall: {recall_test}")
print(f"F1 Score: {f1_test}")

#overfitting is when we do good on training and much worse on testing it means that we memorized the training examples

#plot confusion matrix
confusion_test = confusion_matrix(y_test, y_pred_test)

plt.figure(figsize=(5, 5))
sns.heatmap(confusion_matrix(y_test, y_pred_test), annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for Test Data')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

print("Classification Report for Logistic Regression Test Data:")
print(classification_report(y_test, y_pred_test))

"""## **Decision Tree**

"""

dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train_scaled, y_train)

y_pred_train_dt = dt_classifier.predict(X_train_scaled)

accuracy_train_dt = accuracy_score(y_train, y_pred_train_dt)
precision_train_dt = precision_score(y_train, y_pred_train_dt)
recall_train_dt = recall_score(y_train, y_pred_train_dt)
f1_train_dt = f1_score(y_train, y_pred_train_dt)

print("Decision Tree Training Data Results:")
print(f"Accuracy: {accuracy_train_dt}")
print(f"Precision: {precision_train_dt}")
print(f"Recall: {recall_train_dt}")
print(f"F1 Score: {f1_train_dt}")

y_pred_test_dt = dt_classifier.predict(X_test_scaled)

accuracy_test_dt = accuracy_score(y_test, y_pred_test_dt)
precision_test_dt = precision_score(y_test, y_pred_test_dt)
recall_test_dt = recall_score(y_test, y_pred_test_dt)
f1_test_dt = f1_score(y_test, y_pred_test_dt)

print("Decision Tree Testing Data Results:")
print(f"Accuracy: {accuracy_test_dt}")
print(f"Precision: {precision_test_dt}")
print(f"Recall: {recall_test_dt}")
print(f"F1 Score: {f1_test_dt}")

confusion_test_dt = confusion_matrix(y_test, y_pred_test_dt)

plt.figure(figsize=(5, 5))
sns.heatmap(confusion_test_dt, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for Decision Tree Test Data')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

from sklearn.metrics import classification_report

print("Classification Report for Decision Tree Test Data:")
print(classification_report(y_test, y_pred_test_dt))

"""## **Random Forest**

"""

rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train_scaled, y_train)

y_pred_train_rf = rf_classifier.predict(X_train_scaled)

accuracy_train_rf = accuracy_score(y_train, y_pred_train_rf)
precision_train_rf = precision_score(y_train, y_pred_train_rf)
recall_train_rf = recall_score(y_train, y_pred_train_rf)
f1_train_rf = f1_score(y_train, y_pred_train_rf)

print("Random Forest Training Data Results:")
print(f"Accuracy: {accuracy_train_rf}")
print(f"Precision: {precision_train_rf}")
print(f"Recall: {recall_train_rf}")
print(f"F1 Score: {f1_train_rf}")

y_pred_test_rf = rf_classifier.predict(X_test_scaled)

accuracy_test_rf = accuracy_score(y_test, y_pred_test_rf)
precision_test_rf = precision_score(y_test, y_pred_test_rf)
recall_test_rf = recall_score(y_test, y_pred_test_rf)
f1_test_rf = f1_score(y_test, y_pred_test_rf)

print("Random Forest Testing Data Results:")
print(f"Accuracy: {accuracy_test_rf}")
print(f"Precision: {precision_test_rf}")
print(f"Recall: {recall_test_rf}")
print(f"F1 Score: {f1_test_rf}")

confusion_test_rf = confusion_matrix(y_test, y_pred_test_rf)

plt.figure(figsize=(5, 5))
sns.heatmap(confusion_test_rf, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix for Random Forest Test Data')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

from sklearn.metrics import classification_report

print("Classification Report for Random Forest Test Data:")
print(classification_report(y_test, y_pred_test_rf))